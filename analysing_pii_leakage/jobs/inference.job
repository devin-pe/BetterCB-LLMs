#!/bin/bash

#SBATCH --partition=gpu_a100          # Partition name
#SBATCH --gres=gpu:1                  # Number of GPUs to allocate
#SBATCH --cpus-per-task=12            # Number of CPU cores per task
#SBATCH --gpus=1                      # This line is sometimes optional/redundant depending on your system
#SBATCH --job-name=inference        # Job name (customize)
#SBATCH --ntasks=1                    # Number of tasks
#SBATCH --time=24:00:00               # Time limit hh:mm:ss
#SBATCH --output=/home/dpereira/CB-LLMs/analysing_pii_leakage/work/inference_%A.out      # Standard output (%A expands to job ID)

### --- MODULE SETUP / ENVIRONMENT ---
module purge
module load 2023

# Fix NumPy/PyTorch compatibility issues
export NUMPY_EXPERIMENTAL_ARRAY_FUNCTION=0
export CUDA_LAUNCH_BLOCKING=1

# Set memory management for CUDA
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:256

REPO_ROOT=/home/dpereira/CB-LLMs

# Avoid pip installing potentially conflicting system packages on shared clusters.
# Use PYTHONPATH so local packages are importable without altering system/site packages.
export PYTHONPATH="$REPO_ROOT/analysing_pii_leakage/src:$REPO_ROOT:$PYTHONPATH"

# Optional: check Hugging Face access (doesn't modify environment)
echo "Checking Hugging Face access (this is a lightweight check)..."
python - <<'PY'
try:
	from transformers import AutoTokenizer
	print('Testing Llama3 access...')
	AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3-8B')
	print('✅ Access successful')
except Exception as e:
	print('⚠️  Hugging Face access test failed:', e)
PY

### --- RUN ---
# Run from the package root so relative config paths work reliably
cd "$REPO_ROOT/analysing_pii_leakage"
python examples/evaluate.py --config_path configs/evaluate/pii-inference-llama3.yml

